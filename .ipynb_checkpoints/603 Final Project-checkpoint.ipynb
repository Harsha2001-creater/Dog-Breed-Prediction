{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcb09d25",
   "metadata": {},
   "source": [
    "### Importing Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6db5785",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc72d97",
   "metadata": {},
   "source": [
    "### Creating Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e68559b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyProject\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0537540d",
   "metadata": {},
   "source": [
    "### Initializing a SparkSession with configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "941e5408",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CSVtoMongoDB\") \\\n",
    "    .config(\"spark.mongodb.output.uri\", \"mongodb://localhost:27017/flights.cleaned\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\") \\\n",
    "    .config(\"spark.hadoop.fs.AbstractFileSystem.s3a.impl\", \"org.apache.hadoop.fs.LocalFileSystem\") \\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"in-memory\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7b3564",
   "metadata": {},
   "source": [
    "### Reading Data using Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e53940c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = {\n",
    "    2009: \"C:/Users/msrih/Downloads/UMBC CLASSES/603/Data Sets/2009.csv\",\n",
    "    2010: \"C:/Users/msrih/Downloads/UMBC CLASSES/603/Data Sets/2010.csv\",\n",
    "    2011: \"C:/Users/msrih/Downloads/UMBC CLASSES/603/Data Sets/2011.csv\",\n",
    "    2012: \"C:/Users/msrih/Downloads/UMBC CLASSES/603/Data Sets/2012.csv\",\n",
    "    2013: \"C:/Users/msrih/Downloads/UMBC CLASSES/603/Data Sets/2013.csv\",\n",
    "    2014: \"C:/Users/msrih/Downloads/UMBC CLASSES/603/Data Sets/2014.csv\",\n",
    "    2015: \"C:/Users/msrih/Downloads/UMBC CLASSES/603/Data Sets/2015.csv\",\n",
    "    2016: \"C:/Users/msrih/Downloads/UMBC CLASSES/603/Data Sets/2016.csv\",\n",
    "    2017: \"C:/Users/msrih/Downloads/UMBC CLASSES/603/Data Sets/2017.csv\",\n",
    "    2018: \"C:/Users/msrih/Downloads/UMBC CLASSES/603/Data Sets/2018.csv\"\n",
    "}\n",
    "\n",
    "dfs = {}  # Dictionary to store DataFrames for each year\n",
    "\n",
    "for year, file_path in file_paths.items():\n",
    "    dfs[year] = spark.read.csv(file_path, header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b919a768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------------\n",
      " FL_DATE             | 2009-01-01 \n",
      " OP_CARRIER          | XE         \n",
      " OP_CARRIER_FL_NUM   | 1204       \n",
      " ORIGIN              | DCA        \n",
      " DEST                | EWR        \n",
      " CRS_DEP_TIME        | 1100       \n",
      " DEP_TIME            | 1058.0     \n",
      " DEP_DELAY           | -2.0       \n",
      " TAXI_OUT            | 18.0       \n",
      " WHEELS_OFF          | 1116.0     \n",
      " WHEELS_ON           | 1158.0     \n",
      " TAXI_IN             | 8.0        \n",
      " CRS_ARR_TIME        | 1202       \n",
      " ARR_TIME            | 1206.0     \n",
      " ARR_DELAY           | 4.0        \n",
      " CANCELLED           | 0.0        \n",
      " CANCELLATION_CODE   | NULL       \n",
      " DIVERTED            | 0.0        \n",
      " CRS_ELAPSED_TIME    | 62.0       \n",
      " ACTUAL_ELAPSED_TIME | 68.0       \n",
      " AIR_TIME            | 42.0       \n",
      " DISTANCE            | 199.0      \n",
      " CARRIER_DELAY       | NULL       \n",
      " WEATHER_DELAY       | NULL       \n",
      " NAS_DELAY           | NULL       \n",
      " SECURITY_DELAY      | NULL       \n",
      " LATE_AIRCRAFT_DELAY | NULL       \n",
      " Unnamed: 27         | NULL       \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs[2009].show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd58a8d",
   "metadata": {},
   "source": [
    "### Data Types "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa5fc0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of the loaded dataframes:\n",
      "dfs[2009]: <class 'pyspark.sql.dataframe.DataFrame'>\n",
      "dfs[2010]: <class 'pyspark.sql.dataframe.DataFrame'>\n",
      "dfs[2011]: <class 'pyspark.sql.dataframe.DataFrame'>\n",
      "dfs[2012]: <class 'pyspark.sql.dataframe.DataFrame'>\n",
      "dfs[2013]: <class 'pyspark.sql.dataframe.DataFrame'>\n",
      "dfs[2014]: <class 'pyspark.sql.dataframe.DataFrame'>\n",
      "dfs[2015]: <class 'pyspark.sql.dataframe.DataFrame'>\n",
      "dfs[2016]: <class 'pyspark.sql.dataframe.DataFrame'>\n",
      "dfs[2017]: <class 'pyspark.sql.dataframe.DataFrame'>\n",
      "dfs[2018]: <class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Data type of the loaded dataframes:\")\n",
    "for year, df in dfs.items():\n",
    "    print(f'dfs[{year}]: {type(df)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857c45d9",
   "metadata": {},
   "source": [
    "## Merging all datasets int to One "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3587e79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# List comprehension to extract DataFrame values from the dictionary\n",
    "dfs_list = [dfs[year] for year in dfs]\n",
    "\n",
    "# Concatenating all DataFrames into a single DataFrame\n",
    "df = reduce(DataFrame.unionAll, dfs_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cf730e",
   "metadata": {},
   "source": [
    "### Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1aa1905f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- FL_DATE: date (nullable = true)\n",
      " |-- OP_CARRIER: string (nullable = true)\n",
      " |-- OP_CARRIER_FL_NUM: integer (nullable = true)\n",
      " |-- ORIGIN: string (nullable = true)\n",
      " |-- DEST: string (nullable = true)\n",
      " |-- CRS_DEP_TIME: double (nullable = true)\n",
      " |-- DEP_TIME: double (nullable = true)\n",
      " |-- DEP_DELAY: double (nullable = true)\n",
      " |-- TAXI_OUT: double (nullable = true)\n",
      " |-- WHEELS_OFF: double (nullable = true)\n",
      " |-- WHEELS_ON: double (nullable = true)\n",
      " |-- TAXI_IN: double (nullable = true)\n",
      " |-- CRS_ARR_TIME: double (nullable = true)\n",
      " |-- ARR_TIME: double (nullable = true)\n",
      " |-- ARR_DELAY: double (nullable = true)\n",
      " |-- CANCELLED: double (nullable = true)\n",
      " |-- CANCELLATION_CODE: string (nullable = true)\n",
      " |-- DIVERTED: double (nullable = true)\n",
      " |-- CRS_ELAPSED_TIME: double (nullable = true)\n",
      " |-- ACTUAL_ELAPSED_TIME: double (nullable = true)\n",
      " |-- AIR_TIME: double (nullable = true)\n",
      " |-- DISTANCE: double (nullable = true)\n",
      " |-- CARRIER_DELAY: double (nullable = true)\n",
      " |-- WEATHER_DELAY: double (nullable = true)\n",
      " |-- NAS_DELAY: double (nullable = true)\n",
      " |-- SECURITY_DELAY: double (nullable = true)\n",
      " |-- LATE_AIRCRAFT_DELAY: double (nullable = true)\n",
      " |-- Unnamed: 27: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#schema of df2008\n",
    "df.printSchema()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef54b4a8",
   "metadata": {},
   "source": [
    "## EDA on DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b6bfd5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61556964"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d3e554d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3d5fddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FL_DATE',\n",
       " 'OP_CARRIER',\n",
       " 'OP_CARRIER_FL_NUM',\n",
       " 'ORIGIN',\n",
       " 'DEST',\n",
       " 'CRS_DEP_TIME',\n",
       " 'DEP_TIME',\n",
       " 'DEP_DELAY',\n",
       " 'TAXI_OUT',\n",
       " 'WHEELS_OFF',\n",
       " 'WHEELS_ON',\n",
       " 'TAXI_IN',\n",
       " 'CRS_ARR_TIME',\n",
       " 'ARR_TIME',\n",
       " 'ARR_DELAY',\n",
       " 'CANCELLED',\n",
       " 'CANCELLATION_CODE',\n",
       " 'DIVERTED',\n",
       " 'CRS_ELAPSED_TIME',\n",
       " 'ACTUAL_ELAPSED_TIME',\n",
       " 'AIR_TIME',\n",
       " 'DISTANCE',\n",
       " 'CARRIER_DELAY',\n",
       " 'WEATHER_DELAY',\n",
       " 'NAS_DELAY',\n",
       " 'SECURITY_DELAY',\n",
       " 'LATE_AIRCRAFT_DELAY',\n",
       " 'Unnamed: 27']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b7db2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----------------+--------+--------+-----------------+------------------+-----------------+------------------+------------------+------------------+-----------------+------------------+------------------+-----------------+-------------------+-----------------+--------------------+-----------------+-------------------+------------------+-----------------+-----------------+------------------+------------------+-------------------+-------------------+-----------+\n",
      "|summary|OP_CARRIER|OP_CARRIER_FL_NUM|  ORIGIN|    DEST|     CRS_DEP_TIME|          DEP_TIME|        DEP_DELAY|          TAXI_OUT|        WHEELS_OFF|         WHEELS_ON|          TAXI_IN|      CRS_ARR_TIME|          ARR_TIME|        ARR_DELAY|          CANCELLED|CANCELLATION_CODE|            DIVERTED| CRS_ELAPSED_TIME|ACTUAL_ELAPSED_TIME|          AIR_TIME|         DISTANCE|    CARRIER_DELAY|     WEATHER_DELAY|         NAS_DELAY|     SECURITY_DELAY|LATE_AIRCRAFT_DELAY|Unnamed: 27|\n",
      "+-------+----------+-----------------+--------+--------+-----------------+------------------+-----------------+------------------+------------------+------------------+-----------------+------------------+------------------+-----------------+-------------------+-----------------+--------------------+-----------------+-------------------+------------------+-----------------+-----------------+------------------+------------------+-------------------+-------------------+-----------+\n",
      "|  count|  61556964|         61556964|61556964|61556964|         61556963|          60621241|         60616289|          60593063|          60593068|          60559948|         60559949|          61556962|          60559949|         60435613|           61556964|           973209|            61556964|         61556904|           60438210|          60438211|         61556964|         11390740|          11390740|          11390740|           11390740|           11390740|          0|\n",
      "|   mean|      NULL|2300.241256959976|    NULL|    NULL| 1327.20258853576|  1333.14107175734|9.044878844364755|15.991963337453331| 1356.017720772944| 1473.756664123952|  7.1188165960972|1495.1468870734718|1479.3368845637567|4.700336323220549|0.01580989276859073|             NULL|0.002364590300457313| 136.956422987095|  132.8766195755963|109.77697422248319|786.6761085390762|17.82056020943328|2.6575914295296004|14.403737246219297|0.07852861183733453| 23.283611424718675|       NULL|\n",
      "| stddev|      NULL|1886.557211293661|    NULL|    NULL|474.8935017000135|487.48778347075995|37.13742707634531| 9.311889675522803|488.83695927139553|511.51245075808424|5.304306349800353|495.78675041640867| 515.0970293525413|39.43740171003177| 0.1247394897857509|             NULL|0.048569528013874236|73.37060226293414|   72.8403347342299| 70.67954521240104|593.6700800158058|48.54477871808781|20.600631437679564|29.667233642897568| 2.3879021507066933|  42.74005368072106|       NULL|\n",
      "|    min|        9E|                0|     ABE|     ABE|              1.0|               1.0|           -251.0|               0.0|               1.0|               1.0|              0.0|               1.0|               1.0|           -411.0|                0.0|                A|                 0.0|            -99.0|               11.0|               0.0|             11.0|              0.0|               0.0|               0.0|                0.0|                0.0|       NULL|\n",
      "|    max|        YX|             9855|     YUM|     YUM|           2359.0|            2400.0|           2755.0|             458.0|            2400.0|            2400.0|            414.0|            2400.0|            2400.0|           2692.0|                1.0|                D|                 1.0|           1865.0|              799.0|             723.0|           4983.0|           2439.0|            2692.0|            1848.0|              987.0|             2454.0|       NULL|\n",
      "+-------+----------+-----------------+--------+--------+-----------------+------------------+-----------------+------------------+------------------+------------------+-----------------+------------------+------------------+-----------------+-------------------+-----------------+--------------------+-----------------+-------------------+------------------+-----------------+-----------------+------------------+------------------+-------------------+-------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d741f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for duplicates\n",
    "print(\"Number of duplicate rows:\", df.count() - df.dropDuplicates().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c18fcf",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8934600",
   "metadata": {},
   "source": [
    "#### Data Cleaning Process\n",
    "\n",
    "In this section, we'll outline the steps taken to clean the dataset:\n",
    "\n",
    "1. **Checking for Null Values:** We begin by examining the dataset for any missing values. This step is crucial as missing data can affect the analysis and modeling process.\n",
    "\n",
    "2. **Removing Unnecessary Columns:** Next, we identify and remove any unnecessary columns that do not contribute to our analysis or modeling goals. This helps streamline the dataset and improve efficiency.\n",
    "\n",
    "3. **Removing Null Values:** After identifying missing values, we proceed to handle them appropriately. Depending on the context, we may choose to drop rows or impute missing values using statistical measures.\n",
    "\n",
    "4. **Rechecking for Null Values:** Finally, we perform a final check to ensure that all missing values have been successfully addressed. Ideally, after cleaning, the dataset should be free from any null values, ensuring the integrity of our analysis.\n",
    "\n",
    "By following these steps, we ensure that our dataset is properly cleaned and prepared for further analysis or modeling tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c12f740",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To check the number of null values\n",
    "from pyspark.sql.functions import col, isnan, unix_timestamp, when, count\n",
    "\n",
    "uniondf2 = df.select([\n",
    "    count(when(\n",
    "        col(c).contains('None') | col(c).contains('NULL') | (col(c) == 'NA') | col(c).isNull() | isnan(c),\n",
    "        c)\n",
    "    ).alias(c) if c != 'FL_DATE' else\n",
    "    count(when(\n",
    "        col(c).contains('None') | col(c).contains('NULL') | (col(c) == 'NA') | col(c).isNull() | isnan(unix_timestamp(c)),\n",
    "        c)\n",
    "    ).alias(c)\n",
    "    for c in df.columns\n",
    "])\n",
    "\n",
    "uniondf2.show(vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b43d2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e5ac34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec20862",
   "metadata": {},
   "source": [
    "#### Dropping Unnecesary Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4e4eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the unnecessary columns\n",
    "cols = ('CANCELLATION_CODE', 'DIVERTED', 'CARRIER_DELAY', 'WEATHER_DELAY',\n",
    "                                          'NAS_DELAY', 'SECURITY_DELAY', 'LATE_AIRCRAFT_DELAY',\n",
    "                                          'Unnamed: 27')\n",
    "\n",
    "#Storing the new data into dataframe\n",
    "df=df.drop(*cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a9941e",
   "metadata": {},
   "source": [
    "#### Dropping Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c60bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the null values\n",
    "df= df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc8bfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#re-checking data inconsistencies \n",
    "df.select([\n",
    "    count(when(\n",
    "        col(c).contains('None') | col(c).contains('NULL') | (col(c) == 'NA') | col(c).isNull() | isnan(c),\n",
    "        c)\n",
    "    ).alias(c) if c != 'FL_DATE' else\n",
    "    count(when(\n",
    "        col(c).contains('None') | col(c).contains('NULL') | (col(c) == 'NA') | col(c).isNull() | isnan(unix_timestamp(c)),\n",
    "        c)\n",
    "    ).alias(c)\n",
    "    for c in df.columns\n",
    "]).show(vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dd952a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The shape is {df.count():d} rows by {len(df.columns):d} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4271c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering the data based on the required locations\n",
    "l = ['IAD','BWI','JFK','BOS','DEN','MIA','ORD','SLC','SEA','HOU','ORL','LAS','BNA','CVG','DTW','PIT','LAX','PHX','ATL','DFW','CLT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74e3558",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing the filtered data into df\n",
    "df = df.filter((df.ORIGIN).isin(l) & (df.DEST).isin(l))\n",
    "df.show(vertical = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
